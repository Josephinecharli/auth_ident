{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset reddit (/home/jhuertas/.cache/huggingface/datasets/reddit/default/1.0.0/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6eeb6fa2a9c4fd18dfbd00b16c9716f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('reddit')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "big_dataset = pd.DataFrame({'id': data['author'], 'text': data['body'], 'subreddit': data['subreddit']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = big_dataset.id.value_counts()\n",
    "valid_authors = value_counts[value_counts > 2][1:].index.tolist()\n",
    "big_dataset_valid = big_dataset[big_dataset.id.isin(valid_authors)]\n",
    "in_test = pd.Series(big_dataset_valid.id.unique()).sample(frac=.1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset_train = big_dataset_valid[~big_dataset_valid.id.isin(in_test)]\n",
    "big_dataset_test = big_dataset_valid[big_dataset_valid.id.isin(in_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(big_dataset_train.id.value_counts() <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(big_dataset_test.id.value_counts() <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ilikemesomenofap</th>\n",
       "      <td>I was about to relapse... Have been fantasizin...</td>\n",
       "      <td>NoFap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ilikemesomenofap</th>\n",
       "      <td>One of the most common errors of nofappers,to ...</td>\n",
       "      <td>NoFap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               text subreddit\n",
       "id                                                                           \n",
       "Ilikemesomenofap  I was about to relapse... Have been fantasizin...     NoFap\n",
       "Ilikemesomenofap  One of the most common errors of nofappers,to ...     NoFap"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_dataset_test.set_index('id').loc['Ilikemesomenofap'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "big_dataset_train.to_csv('local_data/reddit_train.csv', index=False, quoting=csv.QUOTE_ALL)\n",
    "big_dataset_test.to_csv('local_data/reddit_test.csv', index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('local_data/reddit_train.csv').sample(frac=1.)\n",
    "test = pd.read_csv('local_data/reddit_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(big_dataset_train.id.value_counts() <= 1))\n",
    "print(sum(big_dataset_test.id.value_counts() <= 1))\n",
    "print(sum(train.id.value_counts() <= 1))\n",
    "print(sum(test.id.value_counts() <= 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1677882</th>\n",
       "      <td>RA_THROWAWY</td>\n",
       "      <td>Update #1: \\n\\nUpdate #2: \\n\\nWe've been datin...</td>\n",
       "      <td>relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288581</th>\n",
       "      <td>RockLoi</td>\n",
       "      <td>I've read a couple of novels like this, but I ...</td>\n",
       "      <td>woahdude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667413</th>\n",
       "      <td>aprilvu</td>\n",
       "      <td>Hey guys, so in March my husband and I bought ...</td>\n",
       "      <td>personalfinance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272215</th>\n",
       "      <td>Tsurii</td>\n",
       "      <td>Of course it's conspiracy theory stuff, the Ma...</td>\n",
       "      <td>WTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659148</th>\n",
       "      <td>OneTrickPony82</td>\n",
       "      <td>You need an assumption about draw rate to calc...</td>\n",
       "      <td>chess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951636</th>\n",
       "      <td>Canadutchian</td>\n",
       "      <td>It has worked that way in the past, but I woul...</td>\n",
       "      <td>ClickerHeroes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537450</th>\n",
       "      <td>Misanthropy-Divine</td>\n",
       "      <td>Regardless of how you perceive someone, we're ...</td>\n",
       "      <td>howtonotgiveafuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256990</th>\n",
       "      <td>carebanana</td>\n",
       "      <td>I hate HP with every fibre of my being. About ...</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724102</th>\n",
       "      <td>Piercemxpx1</td>\n",
       "      <td>So I haven't had any eventful drug stories rec...</td>\n",
       "      <td>Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794095</th>\n",
       "      <td>simmons008</td>\n",
       "      <td>Something I run into frequently when playing a...</td>\n",
       "      <td>Overwatch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1880218 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "1677882         RA_THROWAWY   \n",
       "288581              RockLoi   \n",
       "1667413             aprilvu   \n",
       "1272215              Tsurii   \n",
       "659148       OneTrickPony82   \n",
       "...                     ...   \n",
       "951636         Canadutchian   \n",
       "537450   Misanthropy-Divine   \n",
       "256990           carebanana   \n",
       "1724102         Piercemxpx1   \n",
       "1794095          simmons008   \n",
       "\n",
       "                                                      text          subreddit  \n",
       "1677882  Update #1: \\n\\nUpdate #2: \\n\\nWe've been datin...      relationships  \n",
       "288581   I've read a couple of novels like this, but I ...           woahdude  \n",
       "1667413  Hey guys, so in March my husband and I bought ...    personalfinance  \n",
       "1272215  Of course it's conspiracy theory stuff, the Ma...                WTF  \n",
       "659148   You need an assumption about draw rate to calc...              chess  \n",
       "...                                                    ...                ...  \n",
       "951636   It has worked that way in the past, but I woul...      ClickerHeroes  \n",
       "537450   Regardless of how you perceive someone, we're ...  howtonotgiveafuck  \n",
       "256990   I hate HP with every fibre of my being. About ...          AskReddit  \n",
       "1724102  So I haven't had any eventful drug stories rec...              Drugs  \n",
       "1794095  Something I run into frequently when playing a...          Overwatch  \n",
       "\n",
       "[1880218 rows x 3 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2882383/1149298078.py:5: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('local_data/reddit_train.csv').sample(frac=1.)\n",
      "/tmp/ipykernel_2882383/1149298078.py:6: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv('local_data/reddit_test.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3512f3768f4f43a281efa8265b5770c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7238994bc740f793ba31b97ad33ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8907b5d1a047e28af9727f8a65fa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data import build_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "train = pd.read_csv('local_data/reddit_train.csv').sample(frac=1.)\n",
    "test = pd.read_csv('local_data/reddit_test.csv')\n",
    "\n",
    "train['unique_id'] = train.index.astype(str)\n",
    "test['unique_id'] = test.index.astype(str)\n",
    "\n",
    "BATCH_SIZE = 16384\n",
    "VALID_BATCH_SIZE = 1000\n",
    "CHUNK_SIZE = 512\n",
    "TRAINING_STEPS = 3000\n",
    "VALIDATION_STEPS = 500\n",
    "WARMUP_STEPS = 0\n",
    "\n",
    "train_data = build_dataset(train,\n",
    "                           steps=TRAINING_STEPS*BATCH_SIZE,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           num_workers=8, \n",
    "                           prefetch_factor=8,\n",
    "                           max_len=CHUNK_SIZE,\n",
    "                           tokenizer = AutoTokenizer.from_pretrained('roberta-base'),\n",
    "                           mode='text')\n",
    "test_data = build_dataset(test, \n",
    "                          steps=VALIDATION_STEPS*VALID_BATCH_SIZE, \n",
    "                          batch_size=VALID_BATCH_SIZE, \n",
    "                          num_workers=4, \n",
    "                          prefetch_factor=4, \n",
    "                          max_len=CHUNK_SIZE,\n",
    "                          tokenizer = AutoTokenizer.from_pretrained('roberta-base'),\n",
    "                          mode='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to final_2022-06-08_15-49-38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjahuerta92\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch between the requested accelerator type (GPU) and assigned device type (CPU).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2882383/1723327527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m trainer = Trainer(devices=0,\n\u001b[0m\u001b[1;32m     28\u001b[0m                   \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                   \u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple_trainloader_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         self._accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_processes, devices, tpu_cores, ipus, accelerator, strategy, gpus, gpu_ids, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_device_type_if_training_type_plugin_passed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_accelerator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_devices_if_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m_validate_accelerator_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m# internal error: should not happen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    249\u001b[0m                 \u001b[0;34mf\"Mismatch between the requested accelerator type ({self._accelerator_type})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0;34mf\" and assigned device type ({self._device_type}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch between the requested accelerator type (GPU) and assigned device type (CPU)."
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from model_experimental import (ContrastiveLSTMTransformer,\n",
    "                                )\n",
    "\n",
    "# Name model\n",
    "date_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_name = f'final_{date_time}'\n",
    "print(f'Saving model to {save_name}')\n",
    "\n",
    "wandb.login()\n",
    "wandb_logger = WandbLogger(name=save_name, project=\"author_profiling_reddit\")\n",
    "checkpoint_callback = ModelCheckpoint('model',\n",
    "                                      filename=save_name,\n",
    "                                      monitor=None,\n",
    "                                      every_n_val_epochs=1,\n",
    "                                      )\n",
    "lr_monitor = LearningRateMonitor('step')\n",
    "\n",
    "# Define training arguments\n",
    "trainer = Trainer(devices=0,\n",
    "                  max_steps=3000,\n",
    "                  accelerator='gpu',\n",
    "                  log_every_n_steps=1,\n",
    "                  flush_logs_every_n_steps=500,\n",
    "                  logger=wandb_logger,\n",
    "                  precision=16,\n",
    "                  val_check_interval=250,\n",
    "                  callbacks=[checkpoint_callback, lr_monitor],\n",
    "                  )\n",
    "\n",
    "# Define model\n",
    "base_transformer = AutoModel.from_pretrained('roberta-large')\n",
    "train_model = ContrastiveLSTMTransformer(base_transformer,\n",
    "                                         learning_rate=1e-2,\n",
    "                                         weight_decay=.01,\n",
    "                                         num_warmup_steps=0,\n",
    "                                         num_training_steps=3000,\n",
    "                                         enable_scheduler=True,\n",
    "                                         minibatch_size=256,)\n",
    "\n",
    "trainer.fit(train_model, train_data, test_data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  8 15:50:01 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:37:00.0 Off |                  Off |\n",
      "| 33%   29C    P8    15W / 260W |   1631MiB / 49152MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     Off  | 00000000:86:00.0 Off |                  Off |\n",
      "| 59%   79C    P2   240W / 260W |  41421MiB / 49152MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
