{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset reddit (/home/jhuertas/.cache/huggingface/datasets/reddit/default/1.0.0/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4cd883f8c24b09a159d3d9f8ebc3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('reddit')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "big_dataset = pd.DataFrame({'id': data['author'], 'text': data['body'], 'subreddit': data['subreddit']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate texts\n",
    "clean_dataset = big_dataset.drop_duplicates(subset=[\"text\"], keep=False)\n",
    "\n",
    "# Remove deleted accounts (no author info)\n",
    "clean_dataset = clean_dataset[clean_dataset.id != '[deleted]'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove throwaways\n",
    "throwaways_text = clean_dataset.text.apply(lambda x: 'throwaway' in x.lower()) \n",
    "throwaways_id = clean_dataset.id.apply(lambda x: 'throwaway' in x.lower())\n",
    "lurker_dataset = clean_dataset[throwaways_text | throwaways_id]\n",
    "throwaways = lurker_dataset.id.unique()\n",
    "\n",
    "clean_dataset = clean_dataset[~clean_dataset.id.isin(throwaways)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DejaBoo        825\n",
       "Shaper_pmp     649\n",
       "rand486        573\n",
       "kuvter         549\n",
       "Lots42         450\n",
       "              ... \n",
       "Rucksalot        1\n",
       "G1G4H3RTZ        1\n",
       "aconitine-       1\n",
       "iamknowhere      1\n",
       "an00bisX         1\n",
       "Name: id, Length: 1386057, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = clean_dataset.id.value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16187,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_AUTHORS = 16\n",
    "\n",
    "valid_authors = value_counts[value_counts > N_AUTHORS].index.tolist()\n",
    "minimal_dataset = clean_dataset[clean_dataset.id.isin(valid_authors)]\n",
    "minimal_dataset.id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc64980b605c42d3824b93a87cbca1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/486904 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (867 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (822 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "CHUNK_SIZE = 512\n",
    "\n",
    "def split_data(row):\n",
    "    eid, values = row\n",
    "    input_ids = tokenizer(values.text).input_ids\n",
    "    chunked = [input_ids[chunk: chunk + CHUNK_SIZE] for chunk in range(0, len(input_ids), CHUNK_SIZE)]\n",
    "    decoded_chunked = tokenizer.batch_decode(chunked)\n",
    "    return pd.DataFrame({'id': [values.id]*len(chunked),\n",
    "                         'decoded_text': decoded_chunked})\n",
    "                         \n",
    "with Pool(20) as p:\n",
    "    chunks = list(tqdm(p.imap_unordered(split_data, minimal_dataset.iterrows()),\n",
    "                       total=len(minimal_dataset)))\n",
    "\n",
    "\n",
    "reddit_chunked = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DejaBoo                 1017\n",
       "Shaper_pmp               861\n",
       "kuvter                   600\n",
       "rand486                  583\n",
       "herman_gill              550\n",
       "                        ... \n",
       "1541drive                 13\n",
       "dkmdlb                    13\n",
       "ThatsItGuysShowsOver      12\n",
       "RandomPrecision1          11\n",
       "USAF503                    8\n",
       "Name: id, Length: 16187, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_valid = reddit_chunked[reddit_chunked.decoded_text.apply(len) > 100].drop_duplicates(subset=[\"decoded_text\"], keep=False)\n",
    "value_counts = reddit_valid.id.value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AUTHORS = 16 - 1\n",
    "\n",
    "valid_authors = value_counts[value_counts > N_AUTHORS].index.tolist()\n",
    "big_dataset_valid = reddit_valid[reddit_valid.id.isin(valid_authors)]\n",
    "in_test = pd.Series(big_dataset_valid.id.unique()).sample(frac=.1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dataset_train = big_dataset_valid[~big_dataset_valid.id.isin(in_test)]\n",
    "big_dataset_test = minimal_dataset[minimal_dataset.id.isin(in_test)].drop_duplicates(subset=[\"text\"], keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Perservere</td>\n",
       "      <td>Didn't they lose 6 games in a row? Just becaus...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Duckylicious</td>\n",
       "      <td>If this Plan B is the same as the \"morning aft...</td>\n",
       "      <td>TwoXChromosomes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>BIllyBrooks</td>\n",
       "      <td>This on no way helps, but when I lived in camp...</td>\n",
       "      <td>melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>masasin</td>\n",
       "      <td>I am in mechatronics too. Graduating in a few ...</td>\n",
       "      <td>uwaterloo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Azurphax</td>\n",
       "      <td>Well, I suppose you are getting great color re...</td>\n",
       "      <td>gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847669</th>\n",
       "      <td>Tangerine_Dreams</td>\n",
       "      <td>Hey, awesometacular folks of r/wiiu!\\n\\nI'm so...</td>\n",
       "      <td>wiiu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847672</th>\n",
       "      <td>Snow_Cub</td>\n",
       "      <td>My old computer was stepped on by a rhino (lon...</td>\n",
       "      <td>techsupport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847948</th>\n",
       "      <td>themooseexperience</td>\n",
       "      <td>So I just got the game recently and built myse...</td>\n",
       "      <td>starbound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848009</th>\n",
       "      <td>P2000Camaro</td>\n",
       "      <td>I am a phone salesmen, and I discovered this t...</td>\n",
       "      <td>Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848092</th>\n",
       "      <td>hamfast42</td>\n",
       "      <td>Pretty self explanatory.  Not looking for crea...</td>\n",
       "      <td>asoiafcirclejerk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48376 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "15               Perservere   \n",
       "41             Duckylicious   \n",
       "84              BIllyBrooks   \n",
       "99                  masasin   \n",
       "134                Azurphax   \n",
       "...                     ...   \n",
       "3847669    Tangerine_Dreams   \n",
       "3847672            Snow_Cub   \n",
       "3847948  themooseexperience   \n",
       "3848009         P2000Camaro   \n",
       "3848092           hamfast42   \n",
       "\n",
       "                                                      text         subreddit  \n",
       "15       Didn't they lose 6 games in a row? Just becaus...   leagueoflegends  \n",
       "41       If this Plan B is the same as the \"morning aft...   TwoXChromosomes  \n",
       "84       This on no way helps, but when I lived in camp...         melbourne  \n",
       "99       I am in mechatronics too. Graduating in a few ...         uwaterloo  \n",
       "134      Well, I suppose you are getting great color re...            gaming  \n",
       "...                                                    ...               ...  \n",
       "3847669  Hey, awesometacular folks of r/wiiu!\\n\\nI'm so...              wiiu  \n",
       "3847672  My old computer was stepped on by a rhino (lon...       techsupport  \n",
       "3847948  So I just got the game recently and built myse...         starbound  \n",
       "3848009  I am a phone salesmen, and I discovered this t...           Android  \n",
       "3848092  Pretty self explanatory.  Not looking for crea...  asoiafcirclejerk  \n",
       "\n",
       "[48376 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_value_counts = big_dataset_test.id.value_counts()\n",
    "valid_test = test_value_counts[test_value_counts > N_AUTHORS].index.tolist()\n",
    "big_dataset_test_v2 = big_dataset_test[big_dataset_test.id.isin(valid_test)]\n",
    "big_dataset_test_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(big_dataset_train.id.value_counts() < 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(big_dataset_test_v2.id.value_counts() < 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DejaBoo               1017\n",
       "Shaper_pmp             861\n",
       "kuvter                 600\n",
       "herman_gill            550\n",
       "redweasel              494\n",
       "                      ... \n",
       "estrangedeskimo         16\n",
       "Artegan                 16\n",
       "hired_goon              16\n",
       "Assbutt_Winchester      16\n",
       "sec713                  16\n",
       "Name: id, Length: 14548, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_dataset_train.id.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "big_dataset_train.to_csv('local_data/reddit_train.csv', index=False, quoting=csv.QUOTE_ALL)\n",
    "big_dataset_test_v2.to_csv('local_data/reddit_test.csv', index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>decoded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NightlyReaper</td>\n",
       "      <td>&lt;s&gt;In Mechwarrior Online, I have begun to use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leep420</td>\n",
       "      <td>&lt;s&gt;I take a beta blocker for my heart conditio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wheelman</td>\n",
       "      <td>&lt;s&gt;As an entrepreneur/freelancer (especially a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FrankManic</td>\n",
       "      <td>&lt;s&gt;And that is, hands down, the coolest aspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chrom_ed</td>\n",
       "      <td>&lt;s&gt;So you're saying \"try it, I might not mind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avboden</td>\n",
       "      <td>&lt;s&gt;I completely understand supporting students...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avboden</td>\n",
       "      <td>the exam. \\n\\n\\nEdit 3: THANK YOU! I forgot t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sosuhme</td>\n",
       "      <td>&lt;s&gt;Firstly, I agree with everyone, that hit wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iamtotalcrap</td>\n",
       "      <td>&lt;s&gt;\\nauthor: [Foxcy]( (*1 days*) ``|`` author ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iamtotalcrap</td>\n",
       "      <td>might still celebrate any one holiday religio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>783887 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                       decoded_text\n",
       "0   NightlyReaper  <s>In Mechwarrior Online, I have begun to use ...\n",
       "0         leep420  <s>I take a beta blocker for my heart conditio...\n",
       "0        Wheelman  <s>As an entrepreneur/freelancer (especially a...\n",
       "0      FrankManic  <s>And that is, hands down, the coolest aspect...\n",
       "0        chrom_ed  <s>So you're saying \"try it, I might not mind ...\n",
       "..            ...                                                ...\n",
       "0         avboden  <s>I completely understand supporting students...\n",
       "1         avboden   the exam. \\n\\n\\nEdit 3: THANK YOU! I forgot t...\n",
       "0         sosuhme  <s>Firstly, I agree with everyone, that hit wa...\n",
       "0    iamtotalcrap  <s>\\nauthor: [Foxcy]( (*1 days*) ``|`` author ...\n",
       "1    iamtotalcrap   might still celebrate any one holiday religio...\n",
       "\n",
       "[783887 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2882383/1149298078.py:5: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('local_data/reddit_train.csv').sample(frac=1.)\n",
      "/tmp/ipykernel_2882383/1149298078.py:6: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv('local_data/reddit_test.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3512f3768f4f43a281efa8265b5770c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7238994bc740f793ba31b97ad33ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8907b5d1a047e28af9727f8a65fa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data import build_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "train = pd.read_csv('local_data/reddit_train.csv').sample(frac=1.)\n",
    "test = pd.read_csv('local_data/reddit_test.csv')\n",
    "\n",
    "train['unique_id'] = train.index.astype(str)\n",
    "test['unique_id'] = test.index.astype(str)\n",
    "\n",
    "BATCH_SIZE = 16384\n",
    "VALID_BATCH_SIZE = 1000\n",
    "CHUNK_SIZE = 512\n",
    "TRAINING_STEPS = 3000\n",
    "VALIDATION_STEPS = 500\n",
    "WARMUP_STEPS = 0\n",
    "\n",
    "train_data = build_dataset(train,\n",
    "                           steps=TRAINING_STEPS*BATCH_SIZE,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           num_workers=8, \n",
    "                           prefetch_factor=8,\n",
    "                           max_len=CHUNK_SIZE,\n",
    "                           tokenizer = AutoTokenizer.from_pretrained('roberta-base'),\n",
    "                           mode='text')\n",
    "test_data = build_dataset(test, \n",
    "                          steps=VALIDATION_STEPS*VALID_BATCH_SIZE, \n",
    "                          batch_size=VALID_BATCH_SIZE, \n",
    "                          num_workers=4, \n",
    "                          prefetch_factor=4, \n",
    "                          max_len=CHUNK_SIZE,\n",
    "                          tokenizer = AutoTokenizer.from_pretrained('roberta-base'),\n",
    "                          mode='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBH, I doubt the difficulty rises are going to taper off any time soon. BUT, it could if the price begins to stagnate or fall. Currently there are a number of ASIC producers, as well as a number of up and coming companies with even more efficient designs. This is going to mean the fight to be even more efficient is definitely going to continue to drive up the difficulty, even if the price stays where it is.\n",
      "\n",
      "This is bad news for people wanting to enter the mining market, and existing miners, their slice of the block reward is going to probably get progressively smaller and may drive many miners into the red and out of the game. However conversely, this is fantastic news for Bitcoin in general, multiple chips designs means there is no single point of failure regarding chip design, manufacture, and distribution. It also is fantastic news in the sense that there is incredibly fierce mining competition and this makes it all the harder to disrupt, or try to control the network. If this keeps up, I'm certain in a year, no single country will have any hope of controlling more than half of the hashrate. What's more since chips are being designed, fabbed, and distributed in multiple countries, it would be increasingly difficult for a government to control or limit distribution.\n",
      "\n",
      "tldr; no, high difficulty is here to stay, but this is still fantastic for Bitcoin in general.\n"
     ]
    }
   ],
   "source": [
    "print(train.text.sample(1).tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to final_2022-06-08_15-49-38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjahuerta92\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch between the requested accelerator type (GPU) and assigned device type (CPU).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2882383/1723327527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m trainer = Trainer(devices=0,\n\u001b[0m\u001b[1;32m     28\u001b[0m                   \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                   \u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple_trainloader_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         self._accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_processes, devices, tpu_cores, ipus, accelerator, strategy, gpus, gpu_ids, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_device_type_if_training_type_plugin_passed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_accelerator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_devices_if_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m_validate_accelerator_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m# internal error: should not happen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    249\u001b[0m                 \u001b[0;34mf\"Mismatch between the requested accelerator type ({self._accelerator_type})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0;34mf\" and assigned device type ({self._device_type}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch between the requested accelerator type (GPU) and assigned device type (CPU)."
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from model_experimental import (ContrastiveLSTMTransformer,\n",
    "                                )\n",
    "\n",
    "# Name model\n",
    "date_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_name = f'final_{date_time}'\n",
    "print(f'Saving model to {save_name}')\n",
    "\n",
    "wandb.login()\n",
    "wandb_logger = WandbLogger(name=save_name, project=\"author_profiling_reddit\")\n",
    "checkpoint_callback = ModelCheckpoint('model',\n",
    "                                      filename=save_name,\n",
    "                                      monitor=None,\n",
    "                                      every_n_val_epochs=1,\n",
    "                                      )\n",
    "lr_monitor = LearningRateMonitor('step')\n",
    "\n",
    "# Define training arguments\n",
    "trainer = Trainer(devices=0,\n",
    "                  max_steps=3000,\n",
    "                  accelerator='gpu',\n",
    "                  log_every_n_steps=1,\n",
    "                  flush_logs_every_n_steps=500,\n",
    "                  logger=wandb_logger,\n",
    "                  precision=16,\n",
    "                  val_check_interval=250,\n",
    "                  callbacks=[checkpoint_callback, lr_monitor],\n",
    "                  )\n",
    "\n",
    "# Define model\n",
    "base_transformer = AutoModel.from_pretrained('roberta-large')\n",
    "train_model = ContrastiveLSTMTransformer(base_transformer,\n",
    "                                         learning_rate=1e-2,\n",
    "                                         weight_decay=.01,\n",
    "                                         num_warmup_steps=0,\n",
    "                                         num_training_steps=3000,\n",
    "                                         enable_scheduler=True,\n",
    "                                         minibatch_size=256,)\n",
    "\n",
    "trainer.fit(train_model, train_data, test_data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  8 15:50:01 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:37:00.0 Off |                  Off |\n",
      "| 33%   29C    P8    15W / 260W |   1631MiB / 49152MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     Off  | 00000000:86:00.0 Off |                  Off |\n",
      "| 59%   79C    P2   240W / 260W |  41421MiB / 49152MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>decoded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>------x------</td>\n",
       "      <td>&lt;s&gt;Just found out my boyfriend of 3 years has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>------x------</td>\n",
       "      <td>rying I just thought he was playing a game. We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>------x------</td>\n",
       "      <td>ve made). I am staying with a family friend an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---annon---</td>\n",
       "      <td>&lt;s&gt;Ok so this is my only real badass moment ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---annon---</td>\n",
       "      <td>a retreat focused on healing from sexual abus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629244</th>\n",
       "      <td>zzzzzzzzzzzzzzzzspaf</td>\n",
       "      <td>\\n\\nCom on public security? Like the EU battl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629245</th>\n",
       "      <td>zzzzzzzzzzzzzzzzspaf</td>\n",
       "      <td>with every country to have the authorisation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629246</th>\n",
       "      <td>zzzzzzzzzzzzzzzzspaf</td>\n",
       "      <td>of errors (stuff like 1/2 +3/4 = (1+3)/(2+4) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629247</th>\n",
       "      <td>zzzzzzzzzzzzzzzzspaf</td>\n",
       "      <td>.&lt;\\s&gt;Actually from my understanding (read out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629248</th>\n",
       "      <td>zzzzzzzzzzzzzzzzspaf</td>\n",
       "      <td>on forever (IE you could be unemployed and ke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1629249 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  \\\n",
       "0               ------x------   \n",
       "1               ------x------   \n",
       "2               ------x------   \n",
       "3                 ---annon---   \n",
       "4                 ---annon---   \n",
       "...                       ...   \n",
       "1629244  zzzzzzzzzzzzzzzzspaf   \n",
       "1629245  zzzzzzzzzzzzzzzzspaf   \n",
       "1629246  zzzzzzzzzzzzzzzzspaf   \n",
       "1629247  zzzzzzzzzzzzzzzzspaf   \n",
       "1629248  zzzzzzzzzzzzzzzzspaf   \n",
       "\n",
       "                                              decoded_text  \n",
       "0        <s>Just found out my boyfriend of 3 years has ...  \n",
       "1        rying I just thought he was playing a game. We...  \n",
       "2        ve made). I am staying with a family friend an...  \n",
       "3        <s>Ok so this is my only real badass moment ev...  \n",
       "4         a retreat focused on healing from sexual abus...  \n",
       "...                                                    ...  \n",
       "1629244   \\n\\nCom on public security? Like the EU battl...  \n",
       "1629245   with every country to have the authorisation ...  \n",
       "1629246   of errors (stuff like 1/2 +3/4 = (1+3)/(2+4) ...  \n",
       "1629247  .<\\s>Actually from my understanding (read out ...  \n",
       "1629248   on forever (IE you could be unemployed and ke...  \n",
       "\n",
       "[1629249 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reddit_train = pd.read_csv('local_data/reddit_train.csv')\n",
    "reddit_test = pd.read_csv('local_data/reddit_test.csv')\n",
    "\n",
    "reddit_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "included = reddit_train.decoded_text.apply(len) > 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_train_long = reddit_train[included]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iamtotalcrap          True\n",
       "Death_Star_           True\n",
       "RamsesThePigeon       True\n",
       "DejaBoo               True\n",
       "Shaper_pmp            True\n",
       "                     ...  \n",
       "Ariadenus            False\n",
       "misap                False\n",
       "qs12                 False\n",
       "ffsidonotonlylurk    False\n",
       "Laspimon             False\n",
       "Name: id, Length: 289987, dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_train_long.id.value_counts() > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = reddit_train_long.id.value_counts()\n",
    "valid_authors = value_counts[value_counts >= 10].index.tolist()\n",
    "small_reddit_data = reddit_train_long[reddit_train_long.id.isin(valid_authors)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "small_reddit_data.to_csv('local_data/reddit_train_clean.csv', index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
